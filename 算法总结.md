# 算法总结
### Decision Tree
* ### ID3
多叉树：按选择的特征个数分叉;
信息增益：max（A）：I(D,A) = H(D) - H(D|A);
无法处理连续特征;
信息增益倾向于选择特征个数多的特征;
无法处理缺失值;
没考虑过拟合;
不支持缺失值处理;
* ### C4.5
多叉树，连续节点处为二叉树;
信息增益比;
剪枝：预剪枝，后剪枝;
只能用于分类;
支持缺失值处理;
* ### cart
二叉树，支持缺失值处理;
分类：gini系数;
回归：均方差;
### Adaboost
* 分类：
  * 指数损失函数 + 加法模型 + 前向分步算法
  * 根据当前的学习误差率更新训练样本的权重
  * 样本权重w->分类器样本误差率e->分类器权重α->样本权重w（和前一个总分类器有关）
* 回归：
  * 平方损失函数，拟合残差
  * 根据划分区域，搜索使得损失函数最小的取值 
  
弱学习器不固定
对异常点敏感，样本权重较高

### GBDT
弱学习器固定为cart树;
损失函数不固定;
拟合损失函数的负梯度;
分类：对数似然损失函数（指数损失变为adaboost）;
回归：均方差，绝对损失;
采样是不放回采样;
### xgboost
## 图模型
HMM 生成模型，马尔可夫假设
CRF 判别模型，没有马尔可夫假设（所以容易更好的采纳上下文信息）
### HMM
两个假设：
1）齐次马尔科夫链假设：
隐藏状态之和之前的一个有关
2）观测独立假设：
观测状态之和当前的隐藏状态有关
三个问题：
1）已知参数，观测序列，求观测序列概率，前向后向算法
2）已知观测序列，估计模型参数，EM算法
3）已知参数，观测序列，推测状态序列，维特比算法
### CRF
参数估计，极大似然估计
推测状态序列，维特比算法
特征函数设计
## lr vs svm
概率输出
线性决策
loss不同，基本极大似然估计和距离；
所有数据分布，只要支持向量；
## lr vs 决策树
非线性决策；
直观解释；
容易过拟合；
